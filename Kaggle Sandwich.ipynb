{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# Classification utils\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import *\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "`\n",
    "# Load\n",
    "train = pandas.read_csv('data.csv')\n",
    "test = pandas.read_csv('quiz.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Name Columns (53 total)\n",
    "alphabet = list(string.ascii_lowercase)\n",
    "alphabet2 = alphabet + [l+l for l in alphabet] + ['aaa']\n",
    "\n",
    "train.columns = alphabet2\n",
    "# Leave out label column for test data\n",
    "test.columns = alphabet2[:-1]\n",
    "\n",
    "# Designate Boolean Columns (15 total)\n",
    "boolean_cols = [\n",
    "    'g', 'p', 'q', 's',\n",
    "    'v', 'w', 'y', 'z',\n",
    "    'oo', 'pp', 'qq', 'rr',\n",
    "    'xx', 'yy', 'zz'\n",
    "]\n",
    "\n",
    "# Designate Categorical Columns (16 total)\n",
    "cols = train.columns\n",
    "num_cols = train._get_numeric_data().columns\n",
    "list(set(cols) - set(num_cols))\n",
    "\n",
    "categorical_cols = ['a', 'c', 'e', 'd', 'f',\n",
    " 'uu', 'i', 'k', 'j', 'm',\n",
    " 'l', 'o', 'n', 'ss', 'h',\n",
    " 'tt']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    train[col] = train[col].astype('category')\n",
    "    test[col] = test[col].astype('category')\n",
    "\n",
    "# Designate Numeric Columns (37 total)\n",
    "numeric_cols = ['b', 'g', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y',\n",
    "       'z', 'aa', 'bb', 'cc', 'dd', 'ee', 'ff', 'gg', 'hh', 'ii',\n",
    "       'jj', 'kk', 'll', 'mm', 'nn', 'oo', 'pp', 'qq', 'rr', 'vv',\n",
    "       'ww', 'xx', 'yy', 'zz', 'aaa']\n",
    "\n",
    "numeric_indices = []\n",
    "for i, letter in enumerate(alphabet2):\n",
    "    if letter in numeric_cols:\n",
    "        numeric_indices.append(i)\n",
    "    \n",
    "# [1, 6, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
    "# 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 47, 48, 49, 50, 51, 52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "### Data Preprocessing ###\n",
    "##########################\n",
    "\n",
    "# Normalizing data (just numeric columns)\n",
    "train_std = StandardScaler().fit_transform(train[numeric_cols])\n",
    "test_std = StandardScaler().fit_transform(test[numeric_cols[:-1]])\n",
    "\n",
    "train_std = pandas.DataFrame(data=train_std[0:,0:])\n",
    "test_std = pandas.DataFrame(data=test_std[0:,0:])\n",
    "\n",
    "train_std.columns = numeric_cols\n",
    "# Leave out label column for test data\n",
    "test_std.columns = numeric_cols[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SVD\n",
    "u,s,v = np.linalg.svd(train_std.T)\n",
    "\n",
    "print('SVD: ', u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Eigendecomposition\n",
    "cov_mat = np.cov(train_std.T)\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
    "\n",
    "print('Eigenvectors \\n%s' %eig_vecs)\n",
    "print('\\nEigenvalues \\n%s' %eig_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Explained var\n",
    "tot = sum(eig_vals)\n",
    "var_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "\n",
    "# Graph explained variance of eigenvectors\n",
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    \n",
    "    plt.bar(range(len(var_exp)), var_exp, alpha=0.5, align='center', label='individual explained variance')\n",
    "    plt.step(range(len(cum_var_exp)), cum_var_exp, where='mid', label='cumulative explained variance')\n",
    "    \n",
    "    plt.ylabel('Explained variance ratio')\n",
    "    plt.xlabel('Principal components')\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "# Results:\n",
    "# Determining relevance of eigenvalues of the covariance matrix for just numerical columns\n",
    "# 1st 5 - 42% of variance\n",
    "# 1st 17 - 80%\n",
    "# 1st 20 - 90%  --> use 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PCA Shortcut\n",
    "pca = PCA(n_components=2)\n",
    "Y_sklearn = pca.fit_transform(train_std)\n",
    "print(Y_sklearn)\n",
    "\n",
    "DecisionTreeClassifier().fit()\n",
    "\n",
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    for lab, col in zip(('Iris-setosa', 'Iris-versicolor', 'Iris-virginica'), ('blue', 'red', 'green')):\n",
    "        plt.scatter(Y_sklearn['aaa'==lab, 0], Y_sklearn['aaa'==lab, 1], label=lab, c=col)\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.legend(loc='lower center')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.06538835 -0.05260307 -0.59061626 ..., -0.14997158 -0.02574308\n",
      "  -0.88474326]\n",
      " [-0.06538835 -0.05260307 -0.59061626 ..., -0.14997158 -0.02574308\n",
      "  -0.88474326]\n",
      " [-0.06538835 -0.05260307 -0.59061626 ..., -0.14997158 -0.02574308\n",
      "  -0.88474326]\n",
      " ..., \n",
      " [-0.06538835 -0.05260307 -0.59061626 ..., -0.14997158 -0.02574308\n",
      "   1.13027139]\n",
      " [-0.06538835 -0.05260307 -0.59061626 ..., -0.14997158 -0.02574308\n",
      "   1.13027139]\n",
      " [-0.06538835 -0.05260307 -0.59061626 ..., -0.14997158 -0.02574308\n",
      "  -0.88474326]]\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "### Feature Selection ###\n",
    "#########################\n",
    "\n",
    "# This sections calculates:\n",
    "#   reduced_features_train\n",
    "#   rfe_selected_numeric_cols\n",
    "\n",
    "# Remove features with low variance\n",
    "sel = VarianceThreshold(threshold=(.8))\n",
    "reduced_features_train = sel.fit_transform(train_std)\n",
    "print(reduced_features_train)\n",
    "# print(reduced_features_train)\n",
    "\n",
    "# Use Recursive Feature Selection\n",
    "# Must choose estimator here - todo, try alternatives\n",
    "rfe = RFE(estimator=LogisticRegression(), n_features_to_select=7, step=1)\n",
    "rfe.fit(train[numeric_cols], train['aaa'])\n",
    "# ranking = rfe.ranking_.reshape(train[numeric_cols].shape)\n",
    "\n",
    "# for i in range(37):\n",
    "#     print(i, rfe.ranking_[i])\n",
    "\n",
    "# Results\n",
    "# (0, 27) (1, 16) (2, 14) (3, 4) (4, 12) (5, 17) (6, 9) (7, 6) (8, 5) (9, 20)\n",
    "# (10, 2) (11, 1) (12, 3) (13, 1) (14, 1) (15, 1) (16, 15) (17, 11)\n",
    "# (18, 24) (19, 19) (20, 26) (21, 31) (22, 29) (23, 1) (24, 13) (25, 18) (26, 8)\n",
    "# (27, 10) (28, 21) (29, 1) (30, 7) (31, 28) (32, 30) (33, 23) (34, 22)\n",
    "# (35, 25) (36, 1)\n",
    "\n",
    "# Best numeric cols (rank 1):\n",
    "# 11, 13, 14, 15, 23, 29, 36\n",
    "rfe_selected_numeric_cols = ['l',  'n',  'x',  'p',  'x',  'dd', 'kk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check out coorelation matrix of vars\n",
    "train.corr()\n",
    "\n",
    "# Notable correlations with 'aaa' label:\n",
    "# q 9%\n",
    "# aa 20%\n",
    "# bb 17%\n",
    "# vv 41%\n",
    "# ww 41%\n",
    "\n",
    "coorelated_features = ['q', 'aa', 'bb', 'vv', 'ww']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3    4         5    6    7         8   \\\n",
      "0   1.000008  0.042727 -0.020091 -0.012542  0.0 -0.766397  0.0  0.0  0.003547   \n",
      "1   0.042727  1.000008 -0.011161 -0.013315  0.0 -0.603582  0.0  0.0  0.016942   \n",
      "2  -0.020091 -0.011161  1.000008  0.665596  0.0  0.029754  0.0  0.0  0.120971   \n",
      "3  -0.012542 -0.013315  0.665596  1.000008  0.0  0.023778  0.0  0.0  0.087516   \n",
      "4   0.000000  0.000000  0.000000  0.000000  0.0  0.000000  0.0  0.0  0.000000   \n",
      "5  -0.766397 -0.603582  0.029754  0.023778  0.0  1.000008  0.0  0.0 -0.006010   \n",
      "6   0.000000  0.000000  0.000000  0.000000  0.0  0.000000  0.0  0.0  0.000000   \n",
      "7   0.000000  0.000000  0.000000  0.000000  0.0  0.000000  0.0  0.0  0.000000   \n",
      "8   0.003547  0.016942  0.120971  0.087516  0.0 -0.006010  0.0  0.0  1.000008   \n",
      "9  -0.766397 -0.603582  0.029754  0.023778  0.0  1.000008  0.0  0.0 -0.006010   \n",
      "10  0.000000  0.000000  0.000000  0.000000  0.0  0.000000  0.0  0.0  0.000000   \n",
      "11 -0.006362 -0.002644  0.005586  0.029673  0.0  0.008817  0.0  0.0  0.024638   \n",
      "12 -0.009250  0.019521  0.084376  0.086170  0.0  0.000012  0.0  0.0  0.112986   \n",
      "13 -0.009189 -0.006053 -0.114375 -0.218372  0.0  0.008678  0.0  0.0  0.141228   \n",
      "14 -0.012099 -0.023167  0.325592  0.568624  0.0  0.021929  0.0  0.0 -0.090267   \n",
      "15 -0.017345 -0.022041 -0.166260 -0.250027  0.0  0.026256  0.0  0.0 -0.019077   \n",
      "16  0.000396  0.002039 -0.049430 -0.088792  0.0 -0.002143  0.0  0.0 -0.007027   \n",
      "17 -0.004139 -0.005425 -0.114624 -0.167675  0.0  0.006547  0.0  0.0 -0.047947   \n",
      "18  0.004147 -0.001927 -0.014342 -0.024280  0.0 -0.002212  0.0  0.0 -0.007275   \n",
      "19 -0.001420 -0.000526 -0.032964 -0.058091  0.0  0.001099  0.0  0.0  0.015401   \n",
      "20 -0.004063 -0.010601 -0.068068 -0.126035  0.0  0.009280  0.0  0.0 -0.035451   \n",
      "21  0.005349  0.015540 -0.030530 -0.043247  0.0 -0.011470  0.0  0.0  0.000994   \n",
      "22  0.107389  0.086460 -0.013687 -0.022594  0.0 -0.131956  0.0  0.0 -0.000755   \n",
      "23  0.002001  0.010496 -0.107868 -0.148682  0.0 -0.007531  0.0  0.0  0.024951   \n",
      "24  0.010574  0.012422  0.025927 -0.120556  0.0 -0.017175  0.0  0.0  0.007541   \n",
      "25  0.179869  0.243985 -0.013563 -0.045170  0.0 -0.257863  0.0  0.0  0.018619   \n",
      "26  0.029870  0.014202 -0.031317 -0.059897  0.0 -0.030959  0.0  0.0 -0.003913   \n",
      "27 -0.005040 -0.004054  0.130497  0.110985  0.0  0.006331  0.0  0.0  0.072198   \n",
      "28 -0.002078 -0.001672  0.053814  0.045691  0.0  0.002611  0.0  0.0  0.024091   \n",
      "29 -0.026094 -0.022560  0.628797  0.488204  0.0  0.040833  0.0  0.0  0.074233   \n",
      "30 -0.016500 -0.006019  0.246561  0.329601  0.0  0.018890  0.0  0.0  0.060675   \n",
      "31  0.004013 -0.022484 -0.008331  0.035330  0.0  0.008864  0.0  0.0 -0.087634   \n",
      "32  0.004899 -0.022751 -0.013932  0.029586  0.0  0.008371  0.0  0.0 -0.085639   \n",
      "33  0.005634  0.020929  0.054240  0.035486  0.0 -0.001131  0.0  0.0  0.069723   \n",
      "34  0.005880  0.018753  0.054194  0.036880  0.0  0.001118  0.0  0.0  0.075488   \n",
      "35 -0.001683 -0.001354  0.009992  0.003304  0.0  0.002115  0.0  0.0  0.007720   \n",
      "36  0.004611 -0.034124  0.048597  0.098113  0.0  0.021071  0.0  0.0 -0.052417   \n",
      "\n",
      "          9     ...           27        28        29        30        31  \\\n",
      "0  -0.766397    ...    -0.005040 -0.002078 -0.026094 -0.016500  0.004013   \n",
      "1  -0.603582    ...    -0.004054 -0.001672 -0.022560 -0.006019 -0.022484   \n",
      "2   0.029754    ...     0.130497  0.053814  0.628797  0.246561 -0.008331   \n",
      "3   0.023778    ...     0.110985  0.045691  0.488204  0.329601  0.035330   \n",
      "4   0.000000    ...     0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "5   1.000008    ...     0.006331  0.002611  0.040833  0.018890  0.008864   \n",
      "6   0.000000    ...     0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "7   0.000000    ...     0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "8  -0.006010    ...     0.072198  0.024091  0.074233  0.060675 -0.087634   \n",
      "9   1.000008    ...     0.006331  0.002611  0.040833  0.018890  0.008864   \n",
      "10  0.000000    ...     0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "11  0.008817    ...    -0.018255 -0.033112  0.021855  0.022193  0.017672   \n",
      "12  0.000012    ...     0.050995 -0.013727  0.024213 -0.000497 -0.084219   \n",
      "13  0.008678    ...    -0.050786 -0.020943  0.169436  0.120305 -0.098342   \n",
      "14  0.021929    ...    -0.105721 -0.043597  0.401893  0.364562  0.122805   \n",
      "15  0.026256    ...     0.347603  0.143466 -0.319259 -0.021858 -0.019484   \n",
      "16 -0.002143    ...    -0.010295 -0.004245 -0.080529 -0.262651  0.027365   \n",
      "17  0.006547    ...    -0.020226 -0.008341 -0.164328 -0.523327 -0.012530   \n",
      "18 -0.002212    ...     0.016637  0.005544 -0.025728 -0.070970  0.001569   \n",
      "19  0.001099    ...    -0.007427 -0.003063 -0.055506 -0.185947 -0.007913   \n",
      "20  0.009280    ...    -0.015532 -0.006405 -0.122259 -0.396891 -0.041640   \n",
      "21 -0.011470    ...    -0.004907 -0.002023 -0.041811 -0.129325 -0.021153   \n",
      "22 -0.131956    ...    -0.002997 -0.001236 -0.022370 -0.074172 -0.014172   \n",
      "23 -0.007531    ...    -0.043707 -0.018024 -0.270208 -0.080836 -0.028978   \n",
      "24 -0.017175    ...    -0.014947 -0.006164 -0.126965 -0.003319 -0.013587   \n",
      "25 -0.257863    ...    -0.005996 -0.002472 -0.033978  0.016607 -0.000520   \n",
      "26 -0.030959    ...     0.016670  0.008560 -0.036171  0.007170 -0.019481   \n",
      "27  0.006331    ...     1.000008  0.412383 -0.054125  0.035624 -0.024861   \n",
      "28  0.002611    ...     0.412383  1.000008 -0.022320  0.014598 -0.029381   \n",
      "29  0.040833    ...    -0.054125 -0.022320  1.000008  0.336609 -0.011474   \n",
      "30  0.018890    ...     0.035624  0.014598  0.336609  1.000008  0.018613   \n",
      "31  0.008864    ...    -0.024861 -0.029381 -0.011474  0.018613  1.000008   \n",
      "32  0.008371    ...    -0.024391 -0.029853 -0.014526  0.019320  0.995426   \n",
      "33 -0.001131    ...     0.120872  0.009548 -0.002091 -0.026136 -0.158623   \n",
      "34  0.001118    ...     0.125249 -0.004767 -0.004739 -0.018720 -0.139837   \n",
      "35  0.002115    ...     0.026015  0.008834 -0.003742 -0.001027 -0.024113   \n",
      "36  0.021071    ...     0.061196  0.027418 -0.020447  0.003007  0.411656   \n",
      "\n",
      "          32        33        34        35        36  \n",
      "0   0.004899  0.005634  0.005880 -0.001683  0.004611  \n",
      "1  -0.022751  0.020929  0.018753 -0.001354 -0.034124  \n",
      "2  -0.013932  0.054240  0.054194  0.009992  0.048597  \n",
      "3   0.029586  0.035486  0.036880  0.003304  0.098113  \n",
      "4   0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "5   0.008371 -0.001131  0.001118  0.002115  0.021071  \n",
      "6   0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "7   0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "8  -0.085639  0.069723  0.075488  0.007720 -0.052417  \n",
      "9   0.008371 -0.001131  0.001118  0.002115  0.021071  \n",
      "10  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "11  0.017157  0.142147  0.116600 -0.030909  0.057702  \n",
      "12 -0.081963  0.274791  0.307490 -0.009779 -0.019271  \n",
      "13 -0.092037 -0.057275 -0.058795 -0.006096 -0.206631  \n",
      "14  0.113767 -0.007342 -0.003816 -0.008957  0.171655  \n",
      "15 -0.015282  0.065754  0.072716  0.019634  0.071759  \n",
      "16  0.024271  0.003728  0.001448 -0.001128  0.016098  \n",
      "17 -0.014785  0.037380  0.024009  0.006670  0.010516  \n",
      "18  0.001181  0.004022  0.003219 -0.000943  0.009648  \n",
      "19 -0.009812  0.020917  0.020753  0.006783  0.008228  \n",
      "20 -0.041512  0.024528  0.024801  0.007115 -0.005871  \n",
      "21 -0.021455  0.029465  0.027609 -0.001639 -0.005163  \n",
      "22 -0.015400  0.013194  0.013952 -0.001001  0.002240  \n",
      "23 -0.024031 -0.021729 -0.023280 -0.002681 -0.084423  \n",
      "24 -0.013331 -0.003241 -0.003695 -0.001811 -0.018453  \n",
      "25  0.000484  0.024788  0.026075  0.001772  0.007691  \n",
      "26 -0.023249  0.011356  0.012681 -0.001951  0.006408  \n",
      "27 -0.024391  0.120872  0.125249  0.026015  0.061196  \n",
      "28 -0.029853  0.009548 -0.004767  0.008834  0.027418  \n",
      "29 -0.014526 -0.002091 -0.004739 -0.003742 -0.020447  \n",
      "30  0.019320 -0.026136 -0.018720 -0.001027  0.003007  \n",
      "31  0.995426 -0.158623 -0.139837 -0.024113  0.411656  \n",
      "32  1.000008 -0.162192 -0.142550 -0.024778  0.410965  \n",
      "33 -0.162192  1.000008  0.854092 -0.004394  0.014342  \n",
      "34 -0.142550  0.854092  1.000008  0.121507  0.014729  \n",
      "35 -0.024778 -0.004394  0.121507  1.000008  0.012423  \n",
      "36  0.410965  0.014342  0.014729  0.012423  1.000008  \n",
      "\n",
      "[37 rows x 37 columns]\n",
      "5   0    -0.766397\n",
      "9   0    -0.766397\n",
      "0   5    -0.766397\n",
      "    9    -0.766397\n",
      "1   5    -0.603582\n",
      "    9    -0.603582\n",
      "5   1    -0.603582\n",
      "9   1    -0.603582\n",
      "30  17   -0.523327\n",
      "17  30   -0.523327\n",
      "14  13   -0.475440\n",
      "13  14   -0.475440\n",
      "14  23   -0.401534\n",
      "23  14   -0.401534\n",
      "20  30   -0.396891\n",
      "30  20   -0.396891\n",
      "29  15   -0.319259\n",
      "15  29   -0.319259\n",
      "    14   -0.316786\n",
      "14  15   -0.316786\n",
      "29  23   -0.270208\n",
      "23  29   -0.270208\n",
      "30  16   -0.262651\n",
      "16  30   -0.262651\n",
      "5   25   -0.257863\n",
      "9   25   -0.257863\n",
      "25  9    -0.257863\n",
      "    5    -0.257863\n",
      "3   15   -0.250027\n",
      "15  3    -0.250027\n",
      "            ...   \n",
      "16  16    1.000008\n",
      "1   1     1.000008\n",
      "12  12    1.000008\n",
      "14  14    1.000008\n",
      "28  28    1.000008\n",
      "23  23    1.000008\n",
      "15  15    1.000008\n",
      "32  32    1.000008\n",
      "2   2     1.000008\n",
      "13  13    1.000008\n",
      "29  29    1.000008\n",
      "31  31    1.000008\n",
      "26  26    1.000008\n",
      "8   8     1.000008\n",
      "17  17    1.000008\n",
      "24  24    1.000008\n",
      "3   3     1.000008\n",
      "21  21    1.000008\n",
      "19  19    1.000008\n",
      "30  30    1.000008\n",
      "0   0     1.000008\n",
      "20  20    1.000008\n",
      "5   9     1.000008\n",
      "9   5     1.000008\n",
      "    9     1.000008\n",
      "5   5     1.000008\n",
      "36  36    1.000008\n",
      "22  22    1.000008\n",
      "25  25    1.000008\n",
      "18  18    1.000008\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:8: FutureWarning: order is deprecated, use sort_values(...)\n"
     ]
    }
   ],
   "source": [
    "# Check out covariance matrix of vars\n",
    "cov = np.cov(train_std.T)\n",
    "cov_df = pandas.DataFrame(data=cov)\n",
    "print(cov_df)\n",
    "\n",
    "# Print in sorted order\n",
    "s = cov_df.unstack()\n",
    "so = s.order(kind=\"quicksort\")\n",
    "print(so)\n",
    "\n",
    "# Notable findings:\n",
    "# cols 9, 5 -> 100% coorelation\n",
    "# cols 2 + 3, 29 -> 62%, 48%\n",
    "# cols 1 + 2, 9 -> -76%, -60% \n",
    "# 27, 28 - 41%\n",
    "# 2 + 3, 28 + 29\n",
    "# 31, 32 - 99%\n",
    "# 33, 34, - 89%\n",
    "# 31 + 32, 36 - 41%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Give higher weights to more coorelated variables - todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<class 'sklearn.linear_model.coordinate_descent.Lasso'>, 0.16893695303642042)\n",
      "(<class 'sklearn.linear_model.logistic.LogisticRegression'>, 0.72233500870536449)\n",
      "(<class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>, 0.71696067803291619)\n",
      "(<class 'sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis'>, 0.70242107683716037)\n",
      "(<class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'>, 0.73176308268453727)\n",
      "(<class 'sklearn.neighbors.classification.KNeighborsClassifier'>, 0.81183929568673829)\n",
      "('Best: ', <class 'sklearn.neighbors.classification.KNeighborsClassifier'>)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'write_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ad098a4e501e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m \u001b[0mwrite_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'write_results' is not defined"
     ]
    }
   ],
   "source": [
    "################################\n",
    "### Train + Test Classifiers ###\n",
    "################################\n",
    "\n",
    "# Options:\n",
    "# use train_std (standardized columns)\n",
    "# use reduced_features_train (removed features with little variance)\n",
    "# use train[rfe_selected_numeric_cols] (RFE reduced features)\n",
    "\n",
    "def cross_val(clf, train_data, train_labels):\n",
    "    error_rates = []\n",
    "    for i in range(3):\n",
    "        x_train, x_test, y_train, y_test = train_test_split(train_data,\n",
    "                                                               train_labels,\n",
    "                                                               test_size=0.4)\n",
    "        clf_trained = clf().fit(x_train, y_train)\n",
    "        e = clf_trained.score(x_test, y_test)\n",
    "        error_rates.append(e)\n",
    "    \n",
    "    return np.mean(error_rates)\n",
    "\n",
    "\n",
    "def meta_classify(train, test, train_std, test_std):\n",
    "    '''\n",
    "    Make predictions from `test` features using the classifier that has the lowest error on the `training` features.\n",
    "    Datasets should get correct headers from previous code.\n",
    "    This method depends on the previous `cross_val` method.\n",
    "    '''\n",
    "    \n",
    "    numerical_clf = [\n",
    "        Lasso,\n",
    "#         DecisionTreeClassifier,\n",
    "#         RandomForestClassifier,\n",
    "        LogisticRegression, \n",
    "        LinearDiscriminantAnalysis,\n",
    "        QuadraticDiscriminantAnalysis,\n",
    "        AdaBoostClassifier,\n",
    "        KNeighborsClassifier,  # takes a while...\n",
    "#         SVC,    # takes forever.....\n",
    "    ]\n",
    "    \n",
    "    categorical_clf = [\n",
    "#         GaussianNB,\n",
    "        MultinomialNB,\n",
    "    ]\n",
    "\n",
    "    pred_rates = []\n",
    "    for clf in numerical_clf:\n",
    "        # Keep label out of features\n",
    "        labels = np.array(train['aaa']).astype(int)\n",
    "        e = cross_val(clf, train[numeric_cols[:-1]], labels)\n",
    "        pred_rates.append(e)\n",
    "        print(clf, e)\n",
    "              \n",
    "#     for clf in categorical_clf:\n",
    "#         e = cross_val(clf, train[categorical_cols], train.ix[:,36])\n",
    "#         err_rates.append(e)\n",
    "#         print(clf, e)\n",
    "            \n",
    "    best_clf = numerical_clf[np.argmax(pred_rates)]\n",
    "    print('Best: ', best_clf)\n",
    "    trained = best_clf().fit(train[numeric_cols[:-1]], train['aaa'])\n",
    "    return trained.predict(np.array(test[numeric_cols[:-1]]))\n",
    "    \n",
    "    \n",
    "# print(train.shape)\n",
    "# print(reduced_features_train.shape)\n",
    "# print(train[rfe_selected_numeric_cols])\n",
    "\n",
    "# train, test - pandas\n",
    "# train_std, test_std - pandas\n",
    "\n",
    "\n",
    "preds = meta_classify(train, test, train_std, test_std)\n",
    "preds[preds > 0] = 1\n",
    "preds[preds < 0] = -1\n",
    "preds = preds.astype(int)\n",
    "write_results(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "### Tune params - todo ###\n",
    "##########################\n",
    "\n",
    "DecisionTreeClassifier(max_depth=5),\n",
    "RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "SVC(kernel=\"linear\", C=0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Method to print predicted test labels formatted for kaggle submission\n",
    "def write_results(preds):\n",
    "    with open('test_predictions.csv', 'wb') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writerow(['id', 'Prediction'])\n",
    "        for i, pred in enumerate(preds):\n",
    "            writer.writerow([i+1, pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Misc. functions\n",
    "\n",
    "cross_val(KNeighborsClassifier, np.array(train[numeric_cols[:-1]]), np.array(train['aaa']))\n",
    "# --> 11%\n",
    "\n",
    "knn_clf = KNeighborsClassifier()\n",
    "knn_clf.fit(train[numeric_cols[:-1]], train['aaa'])\n",
    "preds = knn_clf.predict(np.array(test[numeric_cols[:-1]]))\n",
    "write_results(preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
