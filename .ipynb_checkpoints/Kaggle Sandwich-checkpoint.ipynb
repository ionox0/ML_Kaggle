{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# Classification utils\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import *\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "\n",
    "# Load\n",
    "train = pandas.read_csv('data.csv')\n",
    "test = pandas.read_csv('quiz.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Name Columns (53 total)\n",
    "alphabet = list(string.ascii_lowercase)\n",
    "alphabet2 = alphabet + [l+l for l in alphabet] + ['aaa']\n",
    "\n",
    "train.columns = alphabet2\n",
    "# Leave out label column for test data\n",
    "test.columns = alphabet2[:-1]\n",
    "\n",
    "# Designate Boolean Columns (15 total)\n",
    "boolean_cols = [\n",
    "    'g', 'p', 'q', 's',\n",
    "    'v', 'w', 'y', 'z',\n",
    "    'oo', 'pp', 'qq', 'rr',\n",
    "    'xx', 'yy', 'zz'\n",
    "]\n",
    "\n",
    "# Designate Categorical Columns (16 total)\n",
    "cols = train.columns\n",
    "num_cols = train._get_numeric_data().columns\n",
    "list(set(cols) - set(num_cols))\n",
    "\n",
    "categorical_cols = ['a', 'c', 'e', 'd', 'f',\n",
    " 'uu', 'i', 'k', 'j', 'm',\n",
    " 'l', 'o', 'n', 'ss', 'h',\n",
    " 'tt']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    train[col] = train[col].astype('category')\n",
    "    test[col] = test[col].astype('category')\n",
    "\n",
    "# Designate Numeric Columns (37 total)\n",
    "numeric_cols = ['b', 'g', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y',\n",
    "       'z', 'aa', 'bb', 'cc', 'dd', 'ee', 'ff', 'gg', 'hh', 'ii',\n",
    "       'jj', 'kk', 'll', 'mm', 'nn', 'oo', 'pp', 'qq', 'rr', 'vv',\n",
    "       'ww', 'xx', 'yy', 'zz', 'aaa']\n",
    "\n",
    "numeric_indices = []\n",
    "for i, letter in enumerate(alphabet2):\n",
    "    if letter in numeric_cols:\n",
    "        numeric_indices.append(i)\n",
    "    \n",
    "# [1, 6, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
    "# 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 47, 48, 49, 50, 51, 52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "### Data Preprocessing ###\n",
    "##########################\n",
    "\n",
    "# Normalizing data (just numeric columns)\n",
    "train_std = StandardScaler().fit_transform(train[numeric_cols])\n",
    "test_std = StandardScaler().fit_transform(test[numeric_cols[:-1]])\n",
    "\n",
    "train_std = pandas.DataFrame(data=train_std[0:,0:])\n",
    "test_std = pandas.DataFrame(data=test_std[0:,0:])\n",
    "\n",
    "train_std.columns = numeric_cols\n",
    "# Leave out label column for test data\n",
    "test_std.columns = numeric_cols[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SVD\n",
    "u,s,v = np.linalg.svd(train_std.T)\n",
    "\n",
    "print('SVD: ', u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Eigendecomposition\n",
    "cov_mat = np.cov(train_std.T)\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
    "\n",
    "print('Eigenvectors \\n%s' %eig_vecs)\n",
    "print('\\nEigenvalues \\n%s' %eig_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Explained var\n",
    "tot = sum(eig_vals)\n",
    "var_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "\n",
    "# Graph explained variance of eigenvectors\n",
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    \n",
    "    plt.bar(range(len(var_exp)), var_exp, alpha=0.5, align='center', label='individual explained variance')\n",
    "    plt.step(range(len(cum_var_exp)), cum_var_exp, where='mid', label='cumulative explained variance')\n",
    "    \n",
    "    plt.ylabel('Explained variance ratio')\n",
    "    plt.xlabel('Principal components')\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "# Results:\n",
    "# Determining relevance of eigenvalues of the covariance matrix for just numerical columns\n",
    "# 1st 5 - 42% of variance\n",
    "# 1st 17 - 80%\n",
    "# 1st 20 - 90%  --> use 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PCA Shortcut\n",
    "pca = PCA(n_components=2)\n",
    "Y_sklearn = pca.fit_transform(train_std)\n",
    "print(Y_sklearn)\n",
    "\n",
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    for lab, col in zip(('Iris-setosa', 'Iris-versicolor', 'Iris-virginica'), ('blue', 'red', 'green')):\n",
    "        plt.scatter(Y_sklearn['aaa'==lab, 0], Y_sklearn['aaa'==lab, 1], label=lab, c=col)\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.legend(loc='lower center')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "### Feature Selection ###\n",
    "#########################\n",
    "\n",
    "# This sections calculates:\n",
    "#   reduced_features_train\n",
    "#   rfe_selected_numeric_cols\n",
    "\n",
    "# Remove features with low variance\n",
    "sel = VarianceThreshold(threshold=(.8))\n",
    "reduced_features_train = sel.fit_transform(train_std)\n",
    "# print(reduced_features_train)\n",
    "\n",
    "# Use Recursive Feature Selection\n",
    "# Must choose estimator here - todo, try alternatives\n",
    "rfe = RFE(estimator=LogisticRegression(), n_features_to_select=7, step=1)\n",
    "rfe.fit(train[numeric_cols], train['aaa'])\n",
    "# ranking = rfe.ranking_.reshape(train[numeric_cols].shape)\n",
    "\n",
    "# for i in range(37):\n",
    "#     print(i, rfe.ranking_[i])\n",
    "\n",
    "# Results\n",
    "# (0, 27) (1, 16) (2, 14) (3, 4) (4, 12) (5, 17) (6, 9) (7, 6) (8, 5) (9, 20)\n",
    "# (10, 2) (11, 1) (12, 3) (13, 1) (14, 1) (15, 1) (16, 15) (17, 11)\n",
    "# (18, 24) (19, 19) (20, 26) (21, 31) (22, 29) (23, 1) (24, 13) (25, 18) (26, 8)\n",
    "# (27, 10) (28, 21) (29, 1) (30, 7) (31, 28) (32, 30) (33, 23) (34, 22)\n",
    "# (35, 25) (36, 1)\n",
    "\n",
    "# Best numeric cols (rank 1):\n",
    "# 11, 13, 14, 15, 23, 29, 36\n",
    "rfe_selected_numeric_cols = ['l',  'n',  'x',  'p',  'x',  'dd', 'kk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check out coorelation matrix of vars\n",
    "train.corr()\n",
    "\n",
    "# Notable correlations with 'aaa' label:\n",
    "# q 9%\n",
    "# aa 20%\n",
    "# bb 17%\n",
    "# vv 41%\n",
    "# ww 41%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check out covariance matrix of vars\n",
    "cov = np.cov(train_std.T)\n",
    "cov_df = pandas.DataFrame(data=cov)\n",
    "# print(df)\n",
    "\n",
    "# Print in sorted order\n",
    "s = cov_df.unstack()\n",
    "so = s.order(kind=\"quicksort\")\n",
    "print(so)\n",
    "\n",
    "# Notable findings:\n",
    "# cols 9, 5 -> 100% coorelation\n",
    "# cols 2 + 3, 29 -> 62%, 48%\n",
    "# cols 1 + 2, 9 -> -76%, -60% \n",
    "# 27, 28 - 41%\n",
    "# 2 + 3, 28 + 29\n",
    "# 31, 32 - 99%\n",
    "# 33, 34, - 89%\n",
    "# 31 + 32, 36 - 41%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Give higher weights to more coorelated variables - todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<class 'sklearn.linear_model.coordinate_descent.Lasso'>, -5.1887317796447023e-06)\n",
      "(<class 'sklearn.linear_model.logistic.LogisticRegression'>, 0.72203935481751591)\n",
      "(<class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>, 0.71672415492263719)\n",
      "(<class 'sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis'>, 0.56055320127459674)\n",
      "(<class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'>, 0.73223612890509504)\n",
      "(<class 'sklearn.neighbors.classification.KNeighborsClassifier'>, 0.81332413521237801)\n",
      "('Best: ', <class 'sklearn.linear_model.coordinate_descent.Lasso'>)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'write_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-2fb52f1206a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m \u001b[0mwrite_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'write_results' is not defined"
     ]
    }
   ],
   "source": [
    "################################\n",
    "### Train + Test Classifiers ###\n",
    "################################\n",
    "\n",
    "# Options:\n",
    "# use train_std (standardized columns)\n",
    "# use reduced_features_train (removed features with little variance)\n",
    "# use train[rfe_selected_numeric_cols] (RFE reduced features)\n",
    "\n",
    "def cross_val(clf, train_data, train_labels):\n",
    "    error_rates = []\n",
    "    for i in range(3):\n",
    "        x_train, x_test, y_train, y_test = train_test_split(train_data,\n",
    "                                                               train_labels,\n",
    "                                                               test_size=0.4)\n",
    "        clf_trained = clf().fit(x_train, y_train)\n",
    "        e = clf_trained.score(x_test, y_test)\n",
    "        error_rates.append(e)\n",
    "    \n",
    "    return np.mean(error_rates)\n",
    "\n",
    "\n",
    "def meta_classify(train, test, train_std, test_std):\n",
    "    '''\n",
    "    Make predictions from `test` features using the classifier that has the lowest error on the `training` features.\n",
    "    Datasets should get correct headers from previous code.\n",
    "    This method depends on the previous `cross_val` method.\n",
    "    '''\n",
    "    \n",
    "    numerical_clf = [\n",
    "        Lasso,\n",
    "#         DecisionTreeClassifier,\n",
    "#         RandomForestClassifier,\n",
    "        LogisticRegression, \n",
    "        LinearDiscriminantAnalysis,\n",
    "        QuadraticDiscriminantAnalysis,\n",
    "        AdaBoostClassifier,\n",
    "        KNeighborsClassifier,  # takes a while...\n",
    "#         SVC,    # takes forever.....\n",
    "    ]\n",
    "    \n",
    "    categorical_clf = [\n",
    "#         GaussianNB,\n",
    "        MultinomialNB,\n",
    "    ]\n",
    "\n",
    "    err_rates = []\n",
    "    for clf in numerical_clf:\n",
    "        # Keep label out of features\n",
    "        labels = np.array(train_std['aaa']).astype(int)\n",
    "        e = cross_val(clf, train_std[numeric_cols[:-1]], labels)\n",
    "        err_rates.append(e)\n",
    "        print(clf, e)\n",
    "              \n",
    "#     for clf in categorical_clf:\n",
    "#         e = cross_val(clf, train[categorical_cols], train.ix[:,36])\n",
    "#         err_rates.append(e)\n",
    "#         print(clf, e)\n",
    "            \n",
    "    best_clf = numerical_clf[np.argmin(err_rates)]\n",
    "    print('Best: ', best_clf)\n",
    "    trained = best_clf().fit(train_std[numeric_cols[:-1]], train_std['aaa'])\n",
    "    return trained.predict(np.array(test[numeric_cols[:-1]]))\n",
    "    \n",
    "    \n",
    "# print(train.shape)\n",
    "# print(reduced_features_train.shape)\n",
    "# print(train[rfe_selected_numeric_cols])\n",
    "\n",
    "# train, test - pandas\n",
    "# train_std, test_std - pandas\n",
    "\n",
    "\n",
    "preds = meta_classify(train, test, train_std, test_std)\n",
    "preds[preds > 0] = 1\n",
    "preds[preds < 0] = -1\n",
    "preds = preds.astype(int)\n",
    "write_results(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "### Tune params - todo ###\n",
    "##########################\n",
    "\n",
    "DecisionTreeClassifier(max_depth=5),\n",
    "RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "SVC(kernel=\"linear\", C=0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Method to print predicted test labels formatted for kaggle submission\n",
    "def write_results(preds):\n",
    "    with open('test_predictions.csv', 'wb') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writerow(['id', 'Prediction'])\n",
    "        for i, pred in enumerate(preds):\n",
    "            writer.writerow([i+1, pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Misc. functions\n",
    "\n",
    "cross_val(KNeighborsClassifier, np.array(train[numeric_cols[:-1]]), np.array(train['aaa']))\n",
    "# --> 11%\n",
    "\n",
    "knn_clf = KNeighborsClassifier()\n",
    "knn_clf.fit(train[numeric_cols[:-1]], train['aaa'])\n",
    "preds = knn_clf.predict(np.array(test[numeric_cols[:-1]]))\n",
    "write_results(preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
